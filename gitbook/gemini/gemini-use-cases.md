# Gemini Use Cases

\[Gemini: An Overview of Multimodal Use Cases]\([https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro\_multimodal\_use\_cases.ipynb](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro\_multimodal\_use\_cases.ipynb))

\[Gemini 1.5: A workshop in multimodal use cases]\([https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/multimodal\_use\_cases\_workshop.ipynb](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/multimodal\_use\_cases\_workshop.ipynb))

\[Multimodal retail recommendation: using Gemini to recommend items based on images and image reasoning]\([https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retail/multimodal\_retail\_recommendations.ipynb](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retail/multimodal\_retail\_recommendations.ipynb))

\[Multimodal Retrieval Augmented Generation (RAG) using Vertex AI Gemini API]\([https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retrieval-augmented-generation/intro\_multimodal\_rag.ipynb](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retrieval-augmented-generation/intro\_multimodal\_rag.ipynb))

<details>

<summary>Exercise: Apollo 11 Missions</summary>

[https://github.com/udacity/gemini-api-course/blob/main/exercise/solution/Solutions\_Gemini\_Final\_Exercise.ipynb](https://github.com/udacity/gemini-api-course/blob/main/exercise/solution/Solutions\_Gemini\_Final\_Exercise.ipynb)

These are the steps needed to complete the exercise:

1. **Data Preparation:** This step is completed for you. The files are loaded and unzipped in the notebook environment. You can also find a copy of these files in the downloadable Classroom resources.
2. **Data Extraction and Summarization:** Each file type requires some form of extraction before it can be used. For example, the text data is stored in image files, so you'll need to perform Optical Character Recognition (OCR). Then you'll use Gemini to generate summaries using a specialized prompt.
3. **Embedding Generation:** Once you have the text summaries, you'll use an embedding model to generate text embeddings.
4. **Creating a Vector Database:** Vector databases are effective for performing information retrieval tasks with text embeddings. We'll use Chroma DB.
5. **Querying the Vector Database and Retrieval Augmented Generation:** Finally, we'll write code that allows the system to retrieve the most relevant documents (based on their embeddings) and generate a response using the retrieved documents as context.[https://github.com/udacity/gemini-api-course/blob/main/exercise/solution/Solutions\_Gemini\_Final\_Exercise.ipynb](https://github.com/udacity/gemini-api-course/blob/main/exercise/solution/Solutions\_Gemini\_Final\_Exercise.ipynb)

</details>
